{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import matplotlib\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "matplotlib.use('Agg')\r\n",
    "%matplotlib inline\r\n",
    "import datetime\r\n",
    "import statistics\r\n",
    "import seaborn as sns"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from finrl.apps import config\r\n",
    "from finrl.finrl_meta.preprocessor.yahoodownloader import YahooDownloader\r\n",
    "from finrl.finrl_meta.preprocessor.preprocessors import FeatureEngineer, data_split\r\n",
    "from finrl.finrl_meta.env_portfolio_allocation.env_portfolio import StockPortfolioEnv\r\n",
    "from finrl.drl_agents.stablebaselines3.models import DRLAgent\r\n",
    "from finrl.plot import backtest_stats, backtest_plot, get_daily_return, get_baseline,convert_daily_return_to_pyfolio_ts\r\n",
    "from finrl.finrl_meta.data_processor import DataProcessor\r\n",
    "from finrl.finrl_meta.data_processors.processor_yahoofinance import YahooFinanceProcessor\r\n",
    "import sys\r\n",
    "sys.path.append(\"../FinRL-Library\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import random\r\n",
    "import torch\r\n",
    "def set_seed(seed_value):\r\n",
    "    random.seed(seed_value)\r\n",
    "    torch.manual_seed(seed_value)\r\n",
    "    torch.cuda.manual_seed(seed_value)\r\n",
    "    torch.cuda.manual_seed_all(seed_value)\r\n",
    "\r\n",
    "set_seed(100)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gym.utils import seeding\n",
    "import gym\n",
    "from gym import spaces\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "\n",
    "class StockPortfolioEnv(gym.Env):\n",
    "    \"\"\"A single stock trading environment for OpenAI gym\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "        df: DataFrame\n",
    "            input data\n",
    "        stock_dim : int\n",
    "            number of unique stocks\n",
    "        hmax : int\n",
    "            maximum number of shares to trade\n",
    "        initial_amount : int\n",
    "            start money\n",
    "        transaction_cost_pct: float\n",
    "            transaction cost percentage per trade\n",
    "        reward_scaling: float\n",
    "            scaling factor for reward, good for training\n",
    "        state_space: int\n",
    "            the dimension of input features\n",
    "        action_space: int\n",
    "            equals stock dimension\n",
    "        tech_indicator_list: list\n",
    "            a list of technical indicator names\n",
    "        turbulence_threshold: int\n",
    "            a threshold to control risk aversion\n",
    "        day: int\n",
    "            an increment number to control date\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    _sell_stock()\n",
    "        perform sell action based on the sign of the action\n",
    "    _buy_stock()\n",
    "        perform buy action based on the sign of the action\n",
    "    step()\n",
    "        at each step the agent will return actions, then \n",
    "        we will calculate the reward, and return the next observation.\n",
    "    reset()\n",
    "        reset the environment\n",
    "    render()\n",
    "        use render to return other functions\n",
    "    save_asset_memory()\n",
    "        return account value at each time step\n",
    "    save_action_memory()\n",
    "        return actions/positions at each time step\n",
    "        \n",
    "\n",
    "    \"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, \n",
    "                df,\n",
    "                stock_dim,\n",
    "                hmax,\n",
    "                initial_amount,\n",
    "                transaction_cost_pct,\n",
    "                reward_scaling,\n",
    "                state_space,\n",
    "                action_space,\n",
    "                tech_indicator_list,\n",
    "                turbulence_threshold=None,\n",
    "                lookback=252,\n",
    "                day = 0):\n",
    "        #super(StockEnv, self).__init__()\n",
    "        #money = 10 , scope = 1\n",
    "        self.day = day\n",
    "        self.lookback=lookback\n",
    "        self.df = df\n",
    "        self.stock_dim = stock_dim\n",
    "        self.hmax = hmax\n",
    "        self.initial_amount = initial_amount\n",
    "        self.transaction_cost_pct =transaction_cost_pct\n",
    "        self.reward_scaling = reward_scaling\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.tech_indicator_list = tech_indicator_list\n",
    "        self.max_portfolio_value = 0        \n",
    "        self.mdd = 0.0001\n",
    "\n",
    "\n",
    "        # action_space normalization and shape is self.stock_dim\n",
    "        self.action_space = spaces.Box(low = 0, high = 1,shape = (self.action_space,)) \n",
    "        # Shape = (34, 30)\n",
    "        # covariance matrix + technical indicators\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape = (self.state_space+len(self.tech_indicator_list),self.state_space))\n",
    "\n",
    "        # load data from a pandas dataframe\n",
    "        self.data = self.df.loc[self.day,:]\n",
    "        self.covs = self.data['cov_list'].values[0]\n",
    "        self.state =  np.append(np.array(self.covs), [self.data[tech].values.tolist() for tech in self.tech_indicator_list ], axis=0)\n",
    "        self.terminal = False     \n",
    "        self.turbulence_threshold = turbulence_threshold        \n",
    "        # initalize state: inital portfolio return + individual stock return + individual weights\n",
    "        self.portfolio_value = self.initial_amount\n",
    "\n",
    "        # memorize portfolio value each step\n",
    "        self.asset_memory = [self.initial_amount]\n",
    "        # memorize portfolio return each step\n",
    "        self.portfolio_return_memory = [0]\n",
    "        self.actions_memory=[[1/self.stock_dim]*self.stock_dim]\n",
    "        self.date_memory=[self.data.date.unique()[0]]\n",
    "\n",
    "        \n",
    "    def step(self, actions):\n",
    "        # print(self.day)\n",
    "        self.terminal = self.day >= len(self.df.index.unique())-1\n",
    "#         print(actions)\n",
    "\n",
    "        if self.terminal:\n",
    "            df = pd.DataFrame(self.portfolio_return_memory)\n",
    "            df.columns = ['daily_return']\n",
    "            plt.plot(df.daily_return.cumsum(),'r')\n",
    "            plt.savefig('results/cumulative_reward.png')\n",
    "            plt.close()\n",
    "            \n",
    "            plt.plot(self.portfolio_return_memory,'r')\n",
    "            plt.savefig('results/rewards.png')\n",
    "            plt.close()\n",
    "\n",
    "            print(\"=================================\")\n",
    "            print(\"begin_total_asset:{}\".format(self.asset_memory[0]))           \n",
    "            print(\"end_total_asset:{}\".format(self.portfolio_value))\n",
    "\n",
    "            df_daily_return = pd.DataFrame(self.portfolio_return_memory)\n",
    "            df_daily_return.columns = ['daily_return']\n",
    "            if df_daily_return['daily_return'].std() !=0:\n",
    "              sharpe = (252**0.5)*df_daily_return['daily_return'].mean()/ \\\n",
    "                       df_daily_return['daily_return'].std()\n",
    "              print(\"Sharpe: \",sharpe)\n",
    "            print(\"=================================\")\n",
    "            \n",
    "            return self.state, self.reward, self.terminal,{}\n",
    "\n",
    "        else:\n",
    "            #print(\"Model actions: \",actions)\n",
    "            # actions are the portfolio weight\n",
    "            # normalize to sum of 1\n",
    "            #if (np.array(actions) - np.array(actions).min()).sum() != 0:\n",
    "            #  norm_actions = (np.array(actions) - np.array(actions).min()) / (np.array(actions) - np.array(actions).min()).sum()\n",
    "            #else:\n",
    "            #  norm_actions = actions\n",
    "            weights = self.softmax_normalization(actions) \n",
    "            #print(\"Normalized actions: \", weights)\n",
    "            self.actions_memory.append(weights)\n",
    "            last_day_memory = self.data\n",
    "\n",
    "            #load next state\n",
    "            self.day += 1\n",
    "            self.data = self.df.loc[self.day,:]\n",
    "            self.covs = self.data['cov_list'].values[0]\n",
    "            self.state =  np.append(np.array(self.covs), [self.data[tech].values.tolist() for tech in self.tech_indicator_list ], axis=0)\n",
    "            #print(self.state)\n",
    "            # calcualte portfolio return\n",
    "            # individual stocks' return * weight\n",
    "            portfolio_return = sum(((self.data.close.values / last_day_memory.close.values)-1)*weights)\n",
    "            # update portfolio value\n",
    "            new_portfolio_value = self.portfolio_value*(1+portfolio_return)\n",
    "            self.portfolio_value = new_portfolio_value\n",
    "\n",
    "            # save into memory\n",
    "            self.portfolio_return_memory.append(portfolio_return)\n",
    "            self.date_memory.append(self.data.date.unique()[0])            \n",
    "            self.asset_memory.append(new_portfolio_value)\n",
    "\n",
    "            # the reward is the new portfolio value or end portfolo value\n",
    "            self.reward = new_portfolio_value \n",
    "            # self.reward = (new_portfolio_value-self.initial_amount)/self.initial_amount\n",
    "            now_return = (new_portfolio_value-self.initial_amount)/self.initial_amount\n",
    "            ann_return = (1+now_return) ** (252/self.day) -1\n",
    "            if self.max_portfolio_value <  new_portfolio_value :\n",
    "                self.max_portfolio_value = new_portfolio_value\n",
    "            now_dd = 1-(new_portfolio_value/self.max_portfolio_value)\n",
    "            if self.mdd < now_dd:\n",
    "                self.mdd = now_dd    \n",
    "#             self.reward = ann_return+(1/self.mdd)\n",
    "#             print(ann_return)\n",
    "#             print(\"Step reward: \", self.reward)\n",
    "#             self.reward = self.reward*self.reward_scaling\n",
    "\n",
    "        return self.state, self.reward, self.terminal, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.asset_memory = [self.initial_amount]\n",
    "        self.day = 0\n",
    "        self.data = self.df.loc[self.day,:]\n",
    "        # load states\n",
    "        self.covs = self.data['cov_list'].values[0]\n",
    "        self.state =  np.append(np.array(self.covs), [self.data[tech].values.tolist() for tech in self.tech_indicator_list ], axis=0)\n",
    "        self.portfolio_value = self.initial_amount\n",
    "        #self.cost = 0\n",
    "        #self.trades = 0\n",
    "        self.terminal = False \n",
    "        self.portfolio_return_memory = [0]\n",
    "        self.actions_memory=[[1/self.stock_dim]*self.stock_dim]\n",
    "        self.date_memory=[self.data.date.unique()[0]] \n",
    "        return self.state\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        return self.state\n",
    "        \n",
    "    def softmax_normalization(self, actions):\n",
    "        numerator = np.exp(actions)\n",
    "        denominator = np.sum(np.exp(actions))\n",
    "        softmax_output = numerator/denominator\n",
    "        return softmax_output\n",
    "\n",
    "    \n",
    "    def save_asset_memory(self):\n",
    "        date_list = self.date_memory\n",
    "        portfolio_return = self.portfolio_return_memory\n",
    "        #print(len(date_list))\n",
    "        #print(len(asset_list))\n",
    "        df_account_value = pd.DataFrame({'date':date_list,'daily_return':portfolio_return})\n",
    "        return df_account_value\n",
    "\n",
    "    def save_action_memory(self):\n",
    "        # date and close price length must match actions length\n",
    "        date_list = self.date_memory\n",
    "        df_date = pd.DataFrame(date_list)\n",
    "        df_date.columns = ['date']\n",
    "        \n",
    "        action_list = self.actions_memory\n",
    "        df_actions = pd.DataFrame(action_list)\n",
    "        df_actions.columns = self.data.tic.values\n",
    "        df_actions.index = df_date.date\n",
    "        #df_actions = pd.DataFrame({'date':date_list,'actions':action_list})\n",
    "        return df_actions\n",
    "\n",
    "    def _seed(self, seed=None):\n",
    "        seed = 100\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def get_sb_env(self):\n",
    "        e = DummyVecEnv([lambda: self])\n",
    "        obs = e.reset()\n",
    "        return e, obs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_data(etfs):\n",
    "    dp = YahooFinanceProcessor()\n",
    "    df = dp.download_data(start_date = '2008-01-01',\n",
    "                     end_date = '2021-12-31',\n",
    "                     ticker_list = etfs, time_interval='1D')\n",
    "    fe = FeatureEngineer(\n",
    "                    use_technical_indicator=True,\n",
    "                    use_vix=True,\n",
    "                    use_turbulence=True,\n",
    "                    user_defined_feature = False)\n",
    "\n",
    "    df = fe.preprocess_data(df)\n",
    "    # add covariance matrix as states\n",
    "    df=df.sort_values(['date','tic'],ignore_index=True)\n",
    "    df.index = df.date.factorize()[0]\n",
    "\n",
    "    cov_list = []\n",
    "    return_list = []\n",
    "\n",
    "    # look back is one year\n",
    "    lookback=252\n",
    "    for i in range(lookback,len(df.index.unique())):\n",
    "        data_lookback = df.loc[i-lookback:i,:]\n",
    "        price_lookback=data_lookback.pivot_table(index = 'date',columns = 'tic', values = 'close')\n",
    "        return_lookback = price_lookback.pct_change().dropna()\n",
    "        return_list.append(return_lookback)\n",
    "\n",
    "        covs = return_lookback.cov().values \n",
    "        cov_list.append(covs)\n",
    "\n",
    "    df_cov = pd.DataFrame({'date':df.date.unique()[lookback:],'cov_list':cov_list,'return_list':return_list})\n",
    "    df = df.merge(df_cov, on='date')\n",
    "    df = df.sort_values(['date','tic']).reset_index(drop=True)\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def enable_dropout(model):\n",
    "    \"\"\" Function to enable the dropout layers during test-time \"\"\"\n",
    "    for m in model.modules():\n",
    "        if m.__class__.__name__.startswith('Dropout'):\n",
    "            print('enable dropout')\n",
    "            m.train()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def train_rl(df,start_date,end_date,initial_amount=1000000):\n",
    "\n",
    "    \n",
    "    train = data_split(df, start_date,end_date)\n",
    "    stock_dimension = len(train.tic.unique())\n",
    "    state_space = stock_dimension\n",
    "    print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")\n",
    "    \n",
    "    env_kwargs = {\n",
    "        \"hmax\": 100, \n",
    "        \"initial_amount\": initial_amount, \n",
    "        \"transaction_cost_pct\": 0.001, \n",
    "        \"state_space\": state_space, \n",
    "        \"stock_dim\": stock_dimension, \n",
    "        \"tech_indicator_list\": config.TECHNICAL_INDICATORS_LIST, \n",
    "        \"action_space\": stock_dimension, \n",
    "        \"reward_scaling\": 1e-4\n",
    "    }\n",
    "    \n",
    "    e_train_gym = StockPortfolioEnv(df = train, **env_kwargs)\n",
    "    env_train, _ = e_train_gym.get_sb_env()\n",
    "    \n",
    "    agent = DRLAgent(env = env_train)\n",
    "    PPO_PARAMS = {\n",
    "        \"n_steps\": 2048,\n",
    "        \"ent_coef\": 0.005,\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"batch_size\": 128,\n",
    "    }\n",
    "    model_ppo = agent.get_model(\"ppo\",seed=100,model_kwargs = PPO_PARAMS)\n",
    "#     enable_dropout(model_ppo)\n",
    "    trained_ppo = agent.train_model(model=model_ppo, \n",
    "                             tb_log_name='ppo',\n",
    "                             total_timesteps=100000)\n",
    "#     trained_ppo.save('trained_models/trained_ppo.zip')\n",
    "    return trained_ppo"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def trade_rl(df,start_date,end_date,trained_model,initial_amount=1000000):\n",
    "    trade = data_split(df,start_date,end_date)\n",
    "    stock_dimension = len(trade.tic.unique())\n",
    "    state_space = stock_dimension\n",
    "    env_kwargs = {\n",
    "        \"hmax\": 100, \n",
    "        \"initial_amount\": initial_amount, \n",
    "        \"transaction_cost_pct\": 0.001, \n",
    "        \"state_space\": state_space, \n",
    "        \"stock_dim\": stock_dimension, \n",
    "        \"tech_indicator_list\": config.TECHNICAL_INDICATORS_LIST, \n",
    "        \"action_space\": stock_dimension, \n",
    "        \"reward_scaling\": 1e-4\n",
    "    }\n",
    "    e_trade_gym = StockPortfolioEnv(df = trade, **env_kwargs)\n",
    "    df_daily_return, df_actions = DRLAgent.DRL_prediction(model=trained_model, environment = e_trade_gym)\n",
    "    return df_daily_return, df_actions"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "org_etfs = ['ITOT', 'VEU', 'VNQ', 'AGG']\n",
    "org_df = get_data(org_etfs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "org_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "org_etfs = ['ITOT', 'VEU', 'VNQ', 'AGG']\n",
    "ttt_org_df = pd.read_csv('process_data_classic.csv')\n",
    "ttt_org_df.drop([\"Unnamed: 0\"], axis=1, inplace=True)\n",
    "ttt_org_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "org_df.to_csv('process_data_classic.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "org_train_start = '2009-01-01'\n",
    "org_train_end = '2016-01-01'\n",
    "org_initial_amount = 1000000\n",
    "models = []\n",
    "for i in range(1):\n",
    "    print(i)\n",
    "    trained_model = train_rl(org_df,org_train_start,org_train_end,org_initial_amount)\n",
    "    models.append(trained_model)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "org_trade_start = '2016-01-01' \n",
    "org_trade_end = '2021-01-01'\n",
    "trade_action_list = []\n",
    "trade_return_list = []\n",
    "for i in range(1):\n",
    "    trained_model = models[i]\n",
    "#     train_df_daily_return, train_df_actions = trade_rl(org_df,org_train_start,org_train_end,trained_model,org_initial_amount)\n",
    "    trade_df_daily_return, trade_df_actions = trade_rl(org_df,org_trade_start,org_trade_end,trained_model,org_initial_amount)\n",
    "    trade_action_list.append(trade_df_actions)\n",
    "    trade_return_list.append(trade_df_daily_return)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for i in range(5):\n",
    "    print(trade_action_list[i][1050:])\n",
    "    print(trade_return_list[i][1050:])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from pyfolio import timeseries\n",
    "import pyfolio\n",
    "%matplotlib inline\n",
    "\n",
    "baseline_df = get_baseline(\n",
    "        ticker=\"^DJI\", \n",
    "        start = trade_return_list[0].loc[0,'date'],\n",
    "        end = trade_return_list[0].loc[len(trade_return_list[0])-1,'date'])\n",
    "baseline_returns = get_daily_return(baseline_df, value_col_name=\"close\")\n",
    "\n",
    "for i in range(1):\n",
    "    DRL_strat = convert_daily_return_to_pyfolio_ts(trade_return_list[i])\n",
    "    perf_func = timeseries.perf_stats \n",
    "    perf_stats_all = perf_func( returns=DRL_strat, \n",
    "                              factor_returns=DRL_strat, \n",
    "                                positions=None, transactions=None, turnover_denom=\"AGB\")\n",
    "    print(\"==============DRL Strategy Stats===========\")\n",
    "#     print(perf_stats_all)\n",
    "    \n",
    "    with pyfolio.plotting.plotting_context(font_scale=1.1):\n",
    "        pyfolio.create_full_tear_sheet(returns = DRL_strat,\n",
    "                                       benchmark_rets=baseline_returns, set_context=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_df_daily_return, train_df_actions = trade_rl(org_df,'2011-01-01','2016-01-01',trained_model,org_initial_amount)\n",
    "\n",
    "from pyfolio import timeseries\n",
    "import pyfolio\n",
    "%matplotlib inline\n",
    "\n",
    "baseline_df = get_baseline(\n",
    "        ticker=\"^DJI\", \n",
    "        start = train_df_daily_return.loc[0,'date'],\n",
    "        end = train_df_daily_return.loc[len(train_df_daily_return)-1,'date'])\n",
    "baseline_returns = get_daily_return(baseline_df, value_col_name=\"close\")\n",
    "\n",
    "for i in range(1):\n",
    "    DRL_strat = convert_daily_return_to_pyfolio_ts(train_df_daily_return)\n",
    "    perf_func = timeseries.perf_stats \n",
    "    perf_stats_all = perf_func( returns=DRL_strat, \n",
    "                              factor_returns=DRL_strat, \n",
    "                                positions=None, transactions=None, turnover_denom=\"AGB\")\n",
    "    print(\"==============DRL Strategy Stats===========\")\n",
    "#     print(perf_stats_all)\n",
    "    \n",
    "    with pyfolio.plotting.plotting_context(font_scale=1.1):\n",
    "        pyfolio.create_full_tear_sheet(returns = DRL_strat,\n",
    "                                       benchmark_rets=baseline_returns, set_context=False)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_daily_return_nnn, df_actions_nnn = trade_rl(org_df,'2021-01-01','2022-01-01',trained_model,org_initial_amount)\n",
    "\n",
    "from pyfolio import timeseries\n",
    "import pyfolio\n",
    "%matplotlib inline\n",
    "\n",
    "baseline_df = get_baseline(\n",
    "        ticker=\"^DJI\", \n",
    "        start = df_daily_return_nnn.loc[0,'date'],\n",
    "        end = df_daily_return_nnn.loc[len(df_daily_return_nnn)-1,'date'])\n",
    "baseline_returns = get_daily_return(baseline_df, value_col_name=\"close\")\n",
    "\n",
    "for i in range(1):\n",
    "    DRL_strat = convert_daily_return_to_pyfolio_ts(df_daily_return_nnn)\n",
    "    perf_func = timeseries.perf_stats \n",
    "    perf_stats_all = perf_func( returns=DRL_strat, \n",
    "                              factor_returns=DRL_strat, \n",
    "                                positions=None, transactions=None, turnover_denom=\"AGB\")\n",
    "    print(\"==============DRL Strategy Stats===========\")\n",
    "#     print(perf_stats_all)\n",
    "    \n",
    "    with pyfolio.plotting.plotting_context(font_scale=1.1):\n",
    "        pyfolio.create_full_tear_sheet(returns = DRL_strat,\n",
    "                                       benchmark_rets=baseline_returns, set_context=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "vect_dict = {}\n",
    "\n",
    "for j in range(len(trade_action_list[0])):\n",
    "    tmp = []\n",
    "    d = trade_action_list[0].reset_index()['date'][j]\n",
    "    for i in range(5):\n",
    "        trade_return_df = trade_action_list[i].reset_index().T\n",
    "#         print(trade_return_df)\n",
    "        vect = list(trade_return_df[j])[1:]\n",
    "#         print(vect)\n",
    "        tmp.append(vect)\n",
    "    vect_dict[d] = tmp\n",
    "    \n",
    "# vect_dict\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import statistics\n",
    "tmp = np.array(vect_dict['2016-01-05']).T.tolist()\n",
    "for t in tmp:\n",
    "    print(statistics.stdev(t))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import statistics\n",
    "stdev_dict = {}\n",
    "for d in vect_dict:\n",
    "    tmp = np.array(vect_dict[d]).T.tolist()\n",
    "    ttt = []\n",
    "    for t in tmp:\n",
    "        ttt.append(statistics.stdev(t))\n",
    "    avg_stdev = sum(ttt)/len(ttt)\n",
    "    stdev_dict[d] = avg_stdev\n",
    "#     print(ttt,avg_stdev)\n",
    "stdev_list = list(stdev_dict.values())\n",
    "# print(stdev_list)\n",
    "plt.plot(stdev_list)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from itertools import combinations\n",
    "# from math import dist\n",
    "\n",
    "diff_dict = {}\n",
    "max_diff_dict = {}\n",
    "mean_diff_dict = {}\n",
    "for d in vect_dict:\n",
    "    tmp = vect_dict[d]\n",
    "    comb = list(combinations(tmp, 2))\n",
    "    max_dist = 0\n",
    "    tmp = []\n",
    "    for c in comb:\n",
    "        a = np.array(c[0])\n",
    "        b = np.array(c[1])\n",
    "        distance = np.sqrt(np.sum(np.square(a-b)))\n",
    "#         distance = dist(c[0],c[1])\n",
    "        if distance>max_dist:\n",
    "            max_dist = distance\n",
    "        tmp.append(distance)\n",
    "    diff_dict[d] = tmp\n",
    "    mean_diff_dict[d] = sum(tmp)/len(tmp)\n",
    "    max_diff_dict[d] = max_dist\n",
    "max_diff_list = list(max_diff_dict.values())\n",
    "plt.plot(max_diff_list)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "count = 0\n",
    "for m in max_diff_list:\n",
    "    if m > 0.4:\n",
    "        count+=1\n",
    "print(count,len(max_diff_list))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(max(max_diff_list), min(max_diff_list) )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "max_diff_df = pd.DataFrame(list(max_diff_dict.items()),columns=['date','diff'] )\n",
    "# max_diff_df[max_diff_df['diff']>0.4 ]\n",
    "max_diff_df.to_csv('diff.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mean_diff_df = pd.DataFrame(list(mean_diff_dict.items()),columns=['date','diff'] )\n",
    "mean_diff_df.to_csv('diff_mean.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for i in range(5):\n",
    "    trade_action_list[i].to_csv('trade_action_'+str(i)+'.csv')\n",
    "    trade_return_list[i].to_csv('trade_return_'+str(i)+'.csv')\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 看單一agent的輸出分布"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# org_trade_start = '2016-01-01' \n",
    "# org_trade_end = '2021-01-01'\n",
    "org_trade_start = '2009-01-01'\n",
    "org_trade_end = '2016-01-01'\n",
    "trade_action_list = []\n",
    "trade_return_list = []\n",
    "trained_model = models[0]\n",
    "for i in range(250):\n",
    "    print((i))\n",
    "    trade_df_daily_return, trade_df_actions = trade_rl(org_df,org_trade_start,org_trade_end,trained_model,org_initial_amount)\n",
    "    trade_action_list.append(trade_df_actions)\n",
    "    trade_return_list.append(trade_df_daily_return)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "vect_dict = {}\n",
    "\n",
    "for j in range(len(trade_action_list[0])):\n",
    "    tmp = []\n",
    "    d = trade_action_list[0].reset_index()['date'][j]\n",
    "    print(d)\n",
    "    for i in range(250):\n",
    "        trade_return_df = trade_action_list[i].reset_index().T\n",
    "#         print(trade_return_df)\n",
    "        vect = list(trade_return_df[j])[1:]\n",
    "#         print(vect)\n",
    "        tmp.append(vect)\n",
    "    vect_dict[d] = tmp\n",
    "\n",
    "tmp = np.array(vect_dict['2016-01-05']).T.tolist()\n",
    "for t in tmp:\n",
    "    print(statistics.stdev(t),sum(t)/len(t))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# stdev = np.array(stdev_list).T[0]\n",
    "# tmp = np.array(vect_dict['2016-01-05']).T.tolist()\n",
    "for d in vect_dict:\n",
    "    print(d)\n",
    "    tmp = np.array(vect_dict[d]).T.tolist()\n",
    "    fig,ax=plt.subplots(1,4,figsize=(30,8))\n",
    "    for i in range(len(tmp)):\n",
    "        # print(tmp[i])\n",
    "        print(statistics.stdev(tmp[i][:]),sum(tmp[i][:])/len(tmp[i][:]))\n",
    "        # ax[i].hist(tmp[i], density=False, color = 'lightblue', cumulative = False, bins=100)\n",
    "#         ax2[i].boxplot(tmp[i])\n",
    "        sns.distplot(tmp[i][:],ax = ax[i],bins=10 )#,bins=25\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "stdev_dict = {}\n",
    "mean_dict = {}\n",
    "detect = []\n",
    "for d in vect_dict:\n",
    "    tmp = np.array(vect_dict[d]).T.tolist()\n",
    "    ttt1 = []\n",
    "    ttt2 = []\n",
    "    for i in range(len(tmp)):\n",
    "        t = tmp[i]\n",
    "        stdev = statistics.stdev(t[:])\n",
    "        mean = sum(t[:])/len(t[:])\n",
    "        ttt1.append(stdev)#每天4股的stdev\n",
    "        ttt2.append(mean) #mean\n",
    "        if stdev>0.10:\n",
    "#         if stdev<0.08:\n",
    "            detect.append([d,org_etfs[i],stdev,mean])\n",
    "#     avg_stdev = sum(ttt)/len(ttt)\n",
    "#     stdev_dict[d] = avg_stdev\n",
    "#     print(ttt,avg_stdev)\n",
    "    stdev_dict[d] = ttt1\n",
    "    mean_dict[d] = ttt2\n",
    "stdev_list = list(stdev_dict.values())\n",
    "mean_list = list(mean_dict.values())\n",
    "# print(stdev_list)\n",
    "plt.plot(stdev_list)\n",
    "plt.show()\n",
    "plt.plot(mean_list)\n",
    "plt.show()\n",
    "print(detect)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "transp_stdev_list = np.array(stdev_list).T\n",
    "sig = statistics.stdev(transp_stdev_list.reshape(-1))\n",
    "mu = sum(transp_stdev_list.reshape(-1))/len(transp_stdev_list.reshape(-1))\n",
    "print(sig,mu)\n",
    "print(sig*2+mu,sig*2.5+mu,sig*3+mu)#2.3 0.5 0.15%"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "stdev_dict = {}\n",
    "mean_dict = {}\n",
    "detect = []\n",
    "for d in vect_dict:\n",
    "    tmp = np.array(vect_dict[d]).T.tolist()\n",
    "    ttt1 = []\n",
    "    ttt2 = []\n",
    "    for i in range(len(tmp)):\n",
    "        t = tmp[i]\n",
    "        stdev = statistics.stdev(t[:])\n",
    "        mean = sum(t[:])/len(t[:])\n",
    "        ttt1.append(stdev)#每天4股的stdev\n",
    "        ttt2.append(mean) #mean\n",
    "        if stdev>0.10:\n",
    "#         if stdev<0.08:\n",
    "            detect.append([d,org_etfs[i],stdev,mean])\n",
    "#     avg_stdev = sum(ttt)/len(ttt)\n",
    "#     stdev_dict[d] = avg_stdev\n",
    "#     print(ttt,avg_stdev)\n",
    "    stdev_dict[d] = ttt1\n",
    "    mean_dict[d] = ttt2\n",
    "stdev_list = list(stdev_dict.values())\n",
    "mean_list = list(mean_dict.values())\n",
    "# print(stdev_list)\n",
    "plt.plot(stdev_list)\n",
    "plt.show()\n",
    "plt.plot(mean_list)\n",
    "plt.show()\n",
    "print(detect)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "transp_stdev_list = np.array(stdev_list).T\n",
    "t1 = []\n",
    "t2 = []\n",
    "for i in range(len(transp_stdev_list)):\n",
    "    t1.append(statistics.stdev(transp_stdev_list[i]))\n",
    "    t2.append(sum(transp_stdev_list[i])/len(transp_stdev_list[i]))\n",
    "    print(statistics.stdev(transp_stdev_list[i]),sum(transp_stdev_list[i])/len(transp_stdev_list[i]))\n",
    "sig = sum(t1)/len(t1)\n",
    "mu = sum(t2)/len(t2)\n",
    "print(sig,mu,mu+2.5*sig)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sig = statistics.stdev(transp_stdev_list.reshape(-1))\n",
    "mu = sum(transp_stdev_list.reshape(-1))/len(transp_stdev_list.reshape(-1))\n",
    "print(sig,mu)\n",
    "print(sig*2+mu,sig*2.5+mu,sig*3+mu)#2.3 0.5 0.15%"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for i in range(4):\n",
    "    print(sum(np.array(stdev_list).T[i])/len(np.array(stdev_list).T[i]))\n",
    "    plt.plot(np.array(stdev_list).T[i] )\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for i in range(4):\n",
    "    print(sum(np.array(mean_list).T[i])/len(np.array(mean_list).T[i]))\n",
    "    plt.plot(np.array(mean_list).T[i])\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 用google trend找一些時間點"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "### https://www.itread01.com/content/1584432422.html\n",
    "from pytrends.request import TrendReq\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "pytrend = TrendReq(hl='en-US', tz=360)\n",
    "keywords = ['Coronavirus', 'Covid19','Covid-19','Sars']\n",
    "pytrend.build_payload(\n",
    "     kw_list=keywords,\n",
    "     cat=0,\n",
    "     timeframe='2020-02-01 2020-03-31', #today 1-m\n",
    "     geo='US',\n",
    "     gprop='')\n",
    "\n",
    "pprint(pytrend.interest_over_time())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for i in range(4):\n",
    "    print(sum(np.array(stdev_list).T[i])/len(np.array(stdev_list).T[i]))\n",
    "    for j in range(5):\n",
    "        plt.plot(np.array(stdev_list).T[i][j*252:(j+1)*252] )\n",
    "        plt.show()"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "tmp = np.array(vect_dict['2016-01-05']).T.tolist()\n",
    "fig1,ax1=plt.subplots(1,4,figsize=(30,8))\n",
    "# fig2,ax2=plt.subplots(1,4,figsize=(30,8))\n",
    "for i in range(len(tmp)):\n",
    "#     ax1[i].boxplot(tmp[i],vert=False,showmeans=True)\n",
    "#     ax1[i].hist(tmp[i], density=False, color = 'lightblue', cumulative = False, bins=100)\n",
    "    sns.distplot(tmp[i],ax = ax1[i] ,bins=25)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.distplot(tmp[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "np.array(vect_dict['2016-01-05']).T.tolist()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "trade_action_list[0].reset_index()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "class DRLAgentEnsembleNew:\n",
    "    \"\"\"Provides implementations for DRL algorithms\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "        env: gym environment class\n",
    "            user-defined class\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "        get_model()\n",
    "            setup DRL algorithms\n",
    "        train_model()\n",
    "            train DRL algorithms in a train dataset\n",
    "            and output the trained model\n",
    "        DRL_prediction()\n",
    "            make a prediction in a test dataset and get results\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "\n",
    "    def get_model(\n",
    "        self,\n",
    "        model_name,\n",
    "        policy=\"MlpPolicy\",\n",
    "        policy_kwargs=None,\n",
    "        model_kwargs=None,\n",
    "        verbose=1,\n",
    "        seed=None,\n",
    "        tensorboard_log=None,\n",
    "    ):\n",
    "        if model_name not in MODELS:\n",
    "            raise NotImplementedError(\"NotImplementedError\")\n",
    "\n",
    "        if model_kwargs is None:\n",
    "            model_kwargs = MODEL_KWARGS[model_name]\n",
    "\n",
    "        if \"action_noise\" in model_kwargs:\n",
    "            n_actions = self.env.action_space.shape[-1]\n",
    "            model_kwargs[\"action_noise\"] = NOISE[model_kwargs[\"action_noise\"]](\n",
    "                mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions)\n",
    "            )\n",
    "        print(model_kwargs)\n",
    "        model = MODELS[model_name](\n",
    "            policy=policy,\n",
    "            env=self.env,\n",
    "            tensorboard_log=tensorboard_log,\n",
    "            verbose=verbose,\n",
    "            policy_kwargs=policy_kwargs,\n",
    "            seed=seed,\n",
    "            **model_kwargs,\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def train_model(self, model, tb_log_name, total_timesteps=5000):\n",
    "        model = model.learn(\n",
    "            total_timesteps=total_timesteps,\n",
    "            tb_log_name=tb_log_name,\n",
    "            callback=TensorboardCallback(),\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def DRL_prediction(model, environment):\n",
    "        test_env, test_obs = environment.get_sb_env()\n",
    "        \"\"\"make a prediction\"\"\"\n",
    "        account_memory = []\n",
    "        actions_memory = []\n",
    "        test_env.reset()\n",
    "        for i in range(len(environment.df.index.unique())):\n",
    "            action, _states = model.predict(test_obs)\n",
    "            # account_memory = test_env.env_method(method_name=\"save_asset_memory\")\n",
    "            # actions_memory = test_env.env_method(method_name=\"save_action_memory\")\n",
    "            test_obs, rewards, dones, info = test_env.step(action)\n",
    "            if i == (len(environment.df.index.unique()) - 2):\n",
    "                account_memory = test_env.env_method(method_name=\"save_asset_memory\")\n",
    "                actions_memory = test_env.env_method(method_name=\"save_action_memory\")\n",
    "            if dones[0]:\n",
    "                print(\"hit end!\")\n",
    "                break\n",
    "        return account_memory[0], actions_memory[0]\n",
    "    \n",
    "    def run_ensemble_strategy(self, df, model_kwargs, timesteps):\n",
    "        stock_dimension = len(df.tic.unique())\n",
    "        state_space = stock_dimension\n",
    "        e_train_gym = StockPortfolioEnv(df = df, model_kwargs)\n",
    "        env_train, _ = e_train_gym.get_sb_env()\n",
    "        self.env = env_train\n",
    "        PPO_PARAMS = {\n",
    "            \"n_steps\": 2048,\n",
    "            \"ent_coef\": 0.005,\n",
    "            \"learning_rate\": 0.0001,\n",
    "            \"batch_size\": 128,\n",
    "        }\n",
    "        for i in range(10):\n",
    "            model_ppo = self.get_model(\"ppo\",model_kwargs = PPO_PARAMS)\n",
    "            model_ppo = self.train_model(model=model_ppo, tb_log_name='ppo', total_timesteps=100000)\n",
    "            df_daily_return, df_actions = self.DRL_prediction(model=model_ppo, environment = e_train_gym)\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aa]",
   "language": "python",
   "name": "aa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}