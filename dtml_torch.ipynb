{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "import pandas as pd\n",
    "from pandas.core.series import Series\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "import statistics\n",
    "import time\n",
    "import gc\n",
    "import scipy.stats as st\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etfs = ['SPY', 'AGG', 'VNQ', 'VEU', 'ITOT']\n",
    "def get_price_data(train_start='2010-01-01',train_end='2014-12-31'):\n",
    "    filepath='./processed_data.csv'\n",
    "    df = pd.read_csv(filepath)\n",
    "    # etfs = list(set(df.tic))\n",
    "    # train_start = '2010-01-01'\n",
    "    # train_end = '2014-12-31'\n",
    "    price_data = []\n",
    "    for tic in etfs:\n",
    "        tmp_df = df[(df['tic']==tic)&(df['date']>=train_start)&(df['date']<=train_end)]\n",
    "        tmp_df = tmp_df.reset_index(drop=True)\n",
    "        price_data.append(tmp_df)\n",
    "    return price_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_data(windowx=63,windowy=21):\n",
    "    price_data = get_price_data()\n",
    "    all_x_tr = []\n",
    "    all_x_te = []\n",
    "    all_y_tr = []\n",
    "    all_y_te = []\n",
    "    for df in price_data:\n",
    "        x = []\n",
    "        y = []\n",
    "        now_tic = df['tic'][0]\n",
    "        for i in range(len(df)):\n",
    "            if i+windowx+windowy>len(df):\n",
    "                break\n",
    "            tmp_x = np.array(df[df.columns[3:]][i:i+windowx]).tolist()\n",
    "            tmp_y = (df['close'][i+windowx+windowy-1] - df['close'][i+windowx-1])/df['close'][i+windowx-1]\n",
    "            x.append(tmp_x)\n",
    "            y.append(tmp_y)\n",
    "        # all_x[now_tic] = x\n",
    "        # all_y[now_tic] = y\n",
    "        all_x_tr.append(x[:880])\n",
    "        all_x_te.append(x[880:])\n",
    "        if now_tic=='SPY':\n",
    "            continue\n",
    "        all_y_tr.append(y[:880])\n",
    "        all_y_te.append(y[880:])\n",
    "    return all_x_tr,all_y_tr,all_x_te,all_y_te\n",
    "\n",
    "def get_predict_data(pred_start,pred_end,windowx=63):\n",
    "    price_data = get_price_data(train_start=pred_start,train_end=pred_end,windowx=windowx)\n",
    "    for df in price_data:\n",
    "        x = []\n",
    "        y = []\n",
    "        # now_tic = df['tic'][0]\n",
    "        tmp_x = np.array(df[df.columns[3:]][0:len(df)]).tolist()\n",
    "        x.append(tmp_x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention_lstm(nn.Module):\n",
    "    def __init__(self,units,input_shape):\n",
    "        super(Attention_lstm, self).__init__()\n",
    "        self.input_size\n",
    "        self.hidden_dim\n",
    "        self.num_layers\n",
    "        self.lstm_en = nn.LSTM(self.input_size, self.hidden_dim, num_layers = self.num_layers)\n",
    "        self.lstm_de = layers.SimpleRNN(units,dropout=0.2,unroll=True)\n",
    "        self.attention = layers.Attention()\n",
    "        self.fc = layers.Dense(units)\n",
    "        self.out = layers.Dense(1)\n",
    "        \n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.lstm_en(inputs)\n",
    "        x = self.attention([x, x])\n",
    "        x = self.lstm_de(x)\n",
    "        x = self.fc(x)\n",
    "        # out = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,num_heads,input_dim,units):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=input_dim)\n",
    "        self.fc1 = layers.Dense(4*units,activation='relu')\n",
    "        self.fc2 = layers.Dense(1*units,activation='relu')\n",
    "        self.attention = layers.Attention()\n",
    "    def call(self, inputs, training=None):\n",
    "        h_tilde = self.attention([inputs, inputs])\n",
    "        h2 = h_tilde+inputs\n",
    "        h3 = self.fc1(h2)\n",
    "        h3 = self.fc2(h3)\n",
    "        # print('h2',h2.shape,'h3',h3.shape)#h2 (4, 1, 30, 32) h3 (4, 1, 30, 32)\n",
    "        out = tf.math.tanh(h2+h3)\n",
    "        # print(out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self,units,input_shape,num_heads,input_dim):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.units = units\n",
    "        self.attlstm = Attention_lstm(units,input_shape)\n",
    "        self.transf = Transformer(num_heads,input_dim,units)\n",
    "        self.out = layers.Dense(1,activation='sigmoid')\n",
    "        self.layernorm = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        \n",
    "        # temperal attention\n",
    "        hidden_vec = []\n",
    "        # print(inputs.shape)\n",
    "        new_inputs = tf.transpose(inputs, perm=[1,0,2,3])\n",
    "        for i in range(new_inputs.shape[0]):\n",
    "            h = self.attlstm(new_inputs[i])\n",
    "            # h = self.layernorm(h)\n",
    "            hidden_vec.append(h)\n",
    "        # print('hidden_vec',len(hidden_vec)) #hidden_vec 5\n",
    "        # print('hidden_vec[0]',(hidden_vec[0]).shape) #hidden_vec[0] (30, 32)\n",
    "\n",
    "        # context aggregate\n",
    "        new_hidden_vec = []\n",
    "        beta = 0.5\n",
    "        for i in range(1,len(hidden_vec)):\n",
    "            new_hidden_vec.append(hidden_vec[i]+beta*hidden_vec[0])\n",
    "        # print('new_hidden_vec',len(new_hidden_vec)) # new_hidden_vec 4\n",
    "        print('new_hidden_vec[0]',(new_hidden_vec[0]).shape) #new_hidden_vec[0] (30, 64)\n",
    "\n",
    "        # data axis attention\n",
    "        big_h = tf.concat(values=[new_hidden_vec],axis=0)\n",
    "        big_h = tf.reshape(big_h,(-1,len(new_hidden_vec),self.units))\n",
    "        print('big h',big_h.shape)#big h (30,4,64)\n",
    "        hp = self.transf(big_h)\n",
    "        print('hp',hp.shape)#hp (30,4,64)\n",
    "        out = self.out(hp)\n",
    "        print('out',out.shape)#out (30,4,1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aa]",
   "language": "python",
   "name": "aa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
