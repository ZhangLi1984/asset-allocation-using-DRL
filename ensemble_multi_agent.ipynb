{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "\n",
    "from finrl.apps import config\n",
    "from finrl.finrl_meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "from stable_baselines3 import A2C, DDPG, PPO, SAC, TD3\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.noise import (\n",
    "    NormalActionNoise,\n",
    "    OrnsteinUhlenbeckActionNoise,\n",
    ")\n",
    "from finrl.finrl_meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "from finrl.finrl_meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
    "from finrl.finrl_meta.env_portfolio_allocation.env_portfolio import StockPortfolioEnv\n",
    "from finrl.drl_agents.stablebaselines3.models import DRLAgent\n",
    "from finrl.plot import backtest_stats, backtest_plot, get_daily_return, get_baseline,convert_daily_return_to_pyfolio_ts\n",
    "from finrl.finrl_meta.data_processor import DataProcessor\n",
    "from finrl.finrl_meta.data_processors.processor_yahoofinance import YahooFinanceProcessor\n",
    "\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.use('Agg')\n",
    "%matplotlib inline\n",
    "\n",
    "import datetime\n",
    "import statistics\n",
    "import seaborn as sns\n",
    "import pyfolio\n",
    "from pyfolio import timeseries\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../FinRL-Library\")\n",
    "\n",
    "from gym.utils import seeding\n",
    "import gym\n",
    "from gym import spaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "set_seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockPortfolioEnv(gym.Env):\n",
    "    \"\"\"A single stock trading environment for OpenAI gym\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "        df: DataFrame\n",
    "            input data\n",
    "        stock_dim : int\n",
    "            number of unique stocks\n",
    "        hmax : int\n",
    "            maximum number of shares to trade\n",
    "        initial_amount : int\n",
    "            start money\n",
    "        transaction_cost_pct: float\n",
    "            transaction cost percentage per trade\n",
    "        reward_scaling: float\n",
    "            scaling factor for reward, good for training\n",
    "        state_space: int\n",
    "            the dimension of input features\n",
    "        action_space: int\n",
    "            equals stock dimension\n",
    "        tech_indicator_list: list\n",
    "            a list of technical indicator names\n",
    "        turbulence_threshold: int\n",
    "            a threshold to control risk aversion\n",
    "        day: int\n",
    "            an increment number to control date\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    _sell_stock()\n",
    "        perform sell action based on the sign of the action\n",
    "    _buy_stock()\n",
    "        perform buy action based on the sign of the action\n",
    "    step()\n",
    "        at each step the agent will return actions, then \n",
    "        we will calculate the reward, and return the next observation.\n",
    "    reset()\n",
    "        reset the environment\n",
    "    render()\n",
    "        use render to return other functions\n",
    "    save_asset_memory()\n",
    "        return account value at each time step\n",
    "    save_action_memory()\n",
    "        return actions/positions at each time step\n",
    "        \n",
    "\n",
    "    \"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, \n",
    "                df,\n",
    "                stock_dim,\n",
    "                hmax,\n",
    "                initial_amount,\n",
    "                transaction_cost_pct,\n",
    "                reward_scaling,\n",
    "                state_space,\n",
    "                action_space,\n",
    "                tech_indicator_list,\n",
    "                reward_by = 'reward',\n",
    "                turbulence_threshold=None,\n",
    "                lookback=252,\n",
    "                day = 0):\n",
    "        #super(StockEnv, self).__init__()\n",
    "        #money = 10 , scope = 1\n",
    "        self.day = day\n",
    "        self.lookback=lookback\n",
    "        self.df = df\n",
    "        self.stock_dim = stock_dim\n",
    "        self.hmax = hmax\n",
    "        self.initial_amount = initial_amount\n",
    "        self.transaction_cost_pct =transaction_cost_pct\n",
    "        self.reward_scaling = reward_scaling\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.tech_indicator_list = tech_indicator_list\n",
    "        self.max_portfolio_value = 0        \n",
    "        self.mdd = 0.0001\n",
    "        self.reward_by = reward_by\n",
    "        self.return_list = []\n",
    "        self.pre_value = initial_amount\n",
    "\n",
    "\n",
    "        # action_space normalization and shape is self.stock_dim\n",
    "        self.action_space = spaces.Box(low = 0, high = 1,shape = (self.action_space,)) \n",
    "        # Shape = (34, 30)\n",
    "        # covariance matrix + technical indicators\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape = (self.state_space+len(self.tech_indicator_list),self.state_space))\n",
    "\n",
    "        # load data from a pandas dataframe\n",
    "        self.data = self.df.loc[self.day,:]\n",
    "        self.covs = self.data['cov_list'].values[0]\n",
    "        self.state =  np.append(np.array(self.covs), [self.data[tech].values.tolist() for tech in self.tech_indicator_list ], axis=0)\n",
    "        self.terminal = False     \n",
    "        self.turbulence_threshold = turbulence_threshold        \n",
    "        # initalize state: inital portfolio return + individual stock return + individual weights\n",
    "        self.portfolio_value = self.initial_amount\n",
    "\n",
    "        # memorize portfolio value each step\n",
    "        self.asset_memory = [self.initial_amount]\n",
    "        # memorize portfolio return each step\n",
    "        self.portfolio_return_memory = [0]\n",
    "        self.actions_memory=[[1/self.stock_dim]*self.stock_dim]\n",
    "        self.date_memory=[self.data.date.unique()[0]]\n",
    "\n",
    "        \n",
    "    def step(self, actions):\n",
    "        # print(self.day)\n",
    "        self.terminal = self.day >= len(self.df.index.unique())-1\n",
    "#         print(actions)\n",
    "\n",
    "        if self.terminal:\n",
    "            df = pd.DataFrame(self.portfolio_return_memory)\n",
    "            df.columns = ['daily_return']\n",
    "            plt.plot(df.daily_return.cumsum(),'r')\n",
    "            plt.savefig('results/cumulative_reward.png')\n",
    "            plt.close()\n",
    "            \n",
    "            plt.plot(self.portfolio_return_memory,'r')\n",
    "            plt.savefig('results/rewards.png')\n",
    "            plt.close()\n",
    "\n",
    "            print(\"=================================\")\n",
    "            print(\"begin_total_asset:{}\".format(self.asset_memory[0]))           \n",
    "            print(\"end_total_asset:{}\".format(self.portfolio_value))\n",
    "\n",
    "            df_daily_return = pd.DataFrame(self.portfolio_return_memory)\n",
    "            df_daily_return.columns = ['daily_return']\n",
    "            if df_daily_return['daily_return'].std() !=0:\n",
    "              sharpe = (252**0.5)*df_daily_return['daily_return'].mean()/ \\\n",
    "                       df_daily_return['daily_return'].std()\n",
    "              print(\"Sharpe: \",sharpe)\n",
    "            print(\"=================================\")\n",
    "            \n",
    "            return self.state, self.reward, self.terminal,{}\n",
    "\n",
    "        else:\n",
    "            #print(\"Model actions: \",actions)\n",
    "            # actions are the portfolio weight\n",
    "            # normalize to sum of 1\n",
    "            #if (np.array(actions) - np.array(actions).min()).sum() != 0:\n",
    "            #  norm_actions = (np.array(actions) - np.array(actions).min()) / (np.array(actions) - np.array(actions).min()).sum()\n",
    "            #else:\n",
    "            #  norm_actions = actions\n",
    "            weights = self.softmax_normalization(actions) \n",
    "            #print(\"Normalized actions: \", weights)\n",
    "            self.actions_memory.append(weights)\n",
    "            last_day_memory = self.data\n",
    "\n",
    "            #load next state\n",
    "            self.day += 1\n",
    "            self.data = self.df.loc[self.day,:]\n",
    "            self.covs = self.data['cov_list'].values[0]\n",
    "            self.state =  np.append(np.array(self.covs), [self.data[tech].values.tolist() for tech in self.tech_indicator_list ], axis=0)\n",
    "            #print(self.state)\n",
    "            # calcualte portfolio return\n",
    "            # individual stocks' return * weight\n",
    "            portfolio_return = sum(((self.data.close.values / last_day_memory.close.values)-1)*weights)\n",
    "            # update portfolio value\n",
    "            new_portfolio_value = self.portfolio_value*(1+portfolio_return)\n",
    "            self.portfolio_value = new_portfolio_value\n",
    "\n",
    "            # save into memory\n",
    "            self.portfolio_return_memory.append(portfolio_return)\n",
    "            self.date_memory.append(self.data.date.unique()[0])            \n",
    "            self.asset_memory.append(new_portfolio_value)\n",
    "\n",
    "            # the reward is the new portfolio value or end portfolo value\n",
    "            if self.reward_by=='reward':\n",
    "                self.reward = new_portfolio_value \n",
    "            # self.reward = (new_portfolio_value-self.initial_amount)/self.initial_amount\n",
    "            elif self.reward_by=='mdd':\n",
    "                now_return = (new_portfolio_value-self.initial_amount)/self.initial_amount\n",
    "                ann_return = (1+now_return) ** (252/self.day) -1\n",
    "                if self.max_portfolio_value <  new_portfolio_value :\n",
    "                    self.max_portfolio_value = new_portfolio_value\n",
    "                now_dd = 1-(new_portfolio_value/self.max_portfolio_value)\n",
    "                if self.mdd < now_dd:\n",
    "                    self.mdd = now_dd   \n",
    "                self.reward = 1/(self.mdd+0.0001)\n",
    "            elif self.reward_by=='1mstdev':\n",
    "                rrr = (new_portfolio_value-self.pre_value)/self.pre_value\n",
    "                self.return_list.append(rrr)\n",
    "#                 print(self.return_list)\n",
    "                if len(self.return_list)<21:\n",
    "                    self.reward = 0\n",
    "                else:\n",
    "                    self.return_list.pop(0)\n",
    "                    stdev = statistics.stdev(self.return_list)\n",
    "                    if stdev==0:\n",
    "                        self.reward = 1/(stdev+0.0001)\n",
    "                    else:\n",
    "                        self.reward = 1/stdev\n",
    "#             self.reward = ann_return+(1/self.mdd)\n",
    "#             print(ann_return)\n",
    "#             print(\"Step reward: \", self.reward)\n",
    "#             self.reward = self.reward*self.reward_scaling\n",
    "\n",
    "        return self.state, self.reward, self.terminal, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.asset_memory = [self.initial_amount]\n",
    "        self.day = 0\n",
    "        self.data = self.df.loc[self.day,:]\n",
    "        # load states\n",
    "        self.covs = self.data['cov_list'].values[0]\n",
    "        self.state =  np.append(np.array(self.covs), [self.data[tech].values.tolist() for tech in self.tech_indicator_list ], axis=0)\n",
    "        self.portfolio_value = self.initial_amount\n",
    "        #self.cost = 0\n",
    "        #self.trades = 0\n",
    "        self.terminal = False \n",
    "        self.portfolio_return_memory = [0]\n",
    "        self.actions_memory=[[1/self.stock_dim]*self.stock_dim]\n",
    "        self.date_memory=[self.data.date.unique()[0]] \n",
    "        return self.state\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        return self.state\n",
    "        \n",
    "    def softmax_normalization(self, actions):\n",
    "        numerator = np.exp(actions)\n",
    "        denominator = np.sum(np.exp(actions))\n",
    "        softmax_output = numerator/denominator\n",
    "        return softmax_output\n",
    "\n",
    "    \n",
    "    def save_asset_memory(self):\n",
    "        date_list = self.date_memory\n",
    "        portfolio_return = self.portfolio_return_memory\n",
    "        #print(len(date_list))\n",
    "        #print(len(asset_list))\n",
    "        df_account_value = pd.DataFrame({'date':date_list,'daily_return':portfolio_return})\n",
    "        return df_account_value\n",
    "\n",
    "    def save_action_memory(self):\n",
    "        # date and close price length must match actions length\n",
    "        date_list = self.date_memory\n",
    "        df_date = pd.DataFrame(date_list)\n",
    "        df_date.columns = ['date']\n",
    "        \n",
    "        action_list = self.actions_memory\n",
    "        df_actions = pd.DataFrame(action_list)\n",
    "        df_actions.columns = self.data.tic.values\n",
    "        df_actions.index = df_date.date\n",
    "        #df_actions = pd.DataFrame({'date':date_list,'actions':action_list})\n",
    "        return df_actions\n",
    "\n",
    "    def _seed(self, seed=None):\n",
    "        seed = 100\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def get_sb_env(self):\n",
    "        e = DummyVecEnv([lambda: self])\n",
    "        obs = e.reset()\n",
    "        return e, obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(etfs):\n",
    "    dp = YahooFinanceProcessor()\n",
    "    df = dp.download_data(start_date = '2001-01-01',\n",
    "                     end_date = '2021-12-31',\n",
    "                     ticker_list = etfs, time_interval='1D')\n",
    "    fe = FeatureEngineer(\n",
    "                    use_technical_indicator=True,\n",
    "                    use_vix=True,\n",
    "                    use_turbulence=True,\n",
    "                    user_defined_feature = False)\n",
    "\n",
    "    df = fe.preprocess_data(df)\n",
    "    # add covariance matrix as states\n",
    "    df=df.sort_values(['date','tic'],ignore_index=True)\n",
    "    df.index = df.date.factorize()[0]\n",
    "\n",
    "    cov_list = []\n",
    "    return_list = []\n",
    "\n",
    "    # look back is one year\n",
    "    lookback=252\n",
    "    for i in range(lookback,len(df.index.unique())):\n",
    "        data_lookback = df.loc[i-lookback:i,:]\n",
    "        price_lookback=data_lookback.pivot_table(index = 'date',columns = 'tic', values = 'close')\n",
    "        return_lookback = price_lookback.pct_change().dropna()\n",
    "        return_list.append(return_lookback)\n",
    "\n",
    "        covs = return_lookback.cov().values \n",
    "        cov_list.append(covs)\n",
    "\n",
    "    df_cov = pd.DataFrame({'date':df.date.unique()[lookback:],'cov_list':cov_list,'return_list':return_list})\n",
    "    df = df.merge(df_cov, on='date')\n",
    "    df = df.sort_values(['date','tic']).reset_index(drop=True)\n",
    "    print('targets:',df.tic.unique())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rl(df,start_date,end_date,initial_amount=1000000):\n",
    "\n",
    "    \n",
    "    train = data_split(df, start_date,end_date)\n",
    "    stock_dimension = len(train.tic.unique())\n",
    "    state_space = stock_dimension\n",
    "    print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")\n",
    "    \n",
    "    env_kwargs = {\n",
    "        \"hmax\": 100, \n",
    "        \"initial_amount\": initial_amount, \n",
    "        \"transaction_cost_pct\": 0.001, \n",
    "        \"state_space\": state_space, \n",
    "        \"stock_dim\": stock_dimension, \n",
    "        \"tech_indicator_list\": config.TECHNICAL_INDICATORS_LIST, \n",
    "        \"action_space\": stock_dimension, \n",
    "        \"reward_scaling\": 1e-4\n",
    "    }\n",
    "    \n",
    "    e_train_gym = StockPortfolioEnv(df = train, **env_kwargs)\n",
    "    env_train, _ = e_train_gym.get_sb_env()\n",
    "    \n",
    "    agent = DRLAgent(env = env_train)\n",
    "    PPO_PARAMS = {\n",
    "        \"n_steps\": 2048,\n",
    "        \"ent_coef\": 0.005,\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"batch_size\": 128,\n",
    "    }\n",
    "    model_ppo = agent.get_model(\"ppo\",seed=100,model_kwargs = PPO_PARAMS)\n",
    "#     enable_dropout(model_ppo)\n",
    "    trained_ppo = agent.train_model(model=model_ppo, \n",
    "                             tb_log_name='ppo',\n",
    "                             total_timesteps=100000)\n",
    "#     trained_ppo.save('trained_models/trained_ppo.zip')\n",
    "    return trained_ppo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trade_rl(df,start_date,end_date,trained_model,initial_amount=1000000):\n",
    "    trade = data_split(df,start_date,end_date)\n",
    "    stock_dimension = len(trade.tic.unique())\n",
    "    state_space = stock_dimension\n",
    "    env_kwargs = {\n",
    "        \"hmax\": 100, \n",
    "        \"initial_amount\": initial_amount, \n",
    "        \"transaction_cost_pct\": 0.001, \n",
    "        \"state_space\": state_space, \n",
    "        \"stock_dim\": stock_dimension, \n",
    "        \"tech_indicator_list\": config.TECHNICAL_INDICATORS_LIST, \n",
    "        \"action_space\": stock_dimension, \n",
    "        \"reward_scaling\": 1e-4\n",
    "    }\n",
    "    e_trade_gym = StockPortfolioEnv(df = trade, **env_kwargs)\n",
    "    df_daily_return, df_actions = DRLAgent.DRL_prediction(model=trained_model, environment = e_trade_gym)\n",
    "    return df_daily_return, df_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ABC(df_ttt):\n",
    "    df_ttt = df_ttt.reset_index()\n",
    "    df_ttt['day_return'] = 0 \n",
    "    for i in range(len(df_ttt)-1):\n",
    "        df_ttt.loc[i+1,'day_return'] = (df_ttt['Close'][i+1] - df_ttt['Close'][i])/df_ttt['Close'][i]\n",
    "    \n",
    "    df_ttt = df_ttt.fillna(0)\n",
    "    df_ttt['max']=0\n",
    "    s1 = df_ttt['Close']\n",
    "    for i in range(len(df_ttt)):\n",
    "        df_ttt.loc[i,'max'] = s1[0:i+1].max() \n",
    "    \n",
    "    df_ttt['dd'] = 0\n",
    "    df_ttt['dd'] = 1-(df_ttt['Close']/df_ttt['max'])\n",
    "    \n",
    "    mdd = df_ttt['dd'].max()\n",
    "\n",
    "    df_ttt['total_value'] = 1000000\n",
    "    for i in range(1,len(df_ttt)):\n",
    "        df_ttt.loc[i,'total_value'] = df_ttt['total_value'][i-1]*(df_ttt['day_return'][i]+1)\n",
    "\n",
    "    stdev_ttt = statistics.stdev(df_ttt['day_return'])* math.pow( 252, 0.5 )\n",
    "\n",
    "    reward_ttt = (df_ttt['total_value'][len(df_ttt)-1]/df_ttt['total_value'][0])**(252/len(df_ttt))-1\n",
    "\n",
    "    return reward_ttt,stdev_ttt,mdd\n",
    "\n",
    "def get_avg_ABC(comb,start,end):\n",
    "    w = [0.36,0.18,0.06,0.4]\n",
    "    df_ttt_close = pd.read_csv('all_etf_close.csv')#這個檔案要重做\n",
    "    col = comb.copy()\n",
    "    col.insert(0,'Date')\n",
    "    ttttt = df_ttt_close[col]\n",
    "    ttttt = ttttt[(ttttt['Date']>=start) & (ttttt['Date']<end) ]\n",
    "    ttttt['Close'] = 0\n",
    "    c=0\n",
    "    for name in comb:\n",
    "        print(name)\n",
    "        ttttt['Close']+=(ttttt[name]*w[c])\n",
    "        c+=1\n",
    "    ttttt['Close'] /= len(comb)\n",
    "    ttttt = ttttt.reset_index()\n",
    "#     print(ttttt)\n",
    "    reward_ttt,stdev_ttt,mdd = get_ABC(ttttt)\n",
    "    return reward_ttt,stdev_ttt,mdd\n",
    "# get_avg_ABC(['ITOT','VEU','VNQ','AGG'],'2016-01-01','2021-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def detect_uncertainty(models,org_df):\n",
    "    train_action_list = []\n",
    "    train_return_list = []\n",
    "#     trained_model = models[0]\n",
    "    for j in range(250):\n",
    "        print((j))\n",
    "        action_tmp = []\n",
    "        reward_tmp = []\n",
    "        for i in range(len(reward_by_list)):\n",
    "            trained_model = models[i]\n",
    "            train_df_daily_return, train_df_actions = trade_rl(org_df,org_train_start,org_train_end,trained_model,org_initial_amount)\n",
    "            action_tmp.append(train_df_actions)\n",
    "            train_df_daily_return['daily_return']+=1\n",
    "            reward_tmp.append(train_df_daily_return)\n",
    "    \n",
    "        df_concat = pd.concat(action_tmp)\n",
    "        by_row_index = df_concat.groupby(df_concat.index)\n",
    "        final_action = by_row_index.mean()\n",
    "        train_action_list.append(final_action)\n",
    "        \n",
    "        df_concat = pd.concat(reward_tmp)\n",
    "        by_row_index = df_concat.groupby(df_concat.index)\n",
    "        final_reward = by_row_index.mean()-1\n",
    "        train_return_list.append(final_reward)\n",
    "    \n",
    "    df_concat = pd.concat(train_action_list)\n",
    "    by_row_index = df_concat.groupby(df_concat.index)\n",
    "    mean_df = by_row_index.mean()\n",
    "    stdev_df = by_row_index.std()\n",
    "    mean_dict = mean_df.T.to_dict('list')\n",
    "    stdev_dict = stdev_df.T.to_dict('list')\n",
    "    stdev_list = list(stdev_dict.values())\n",
    "    mean_list = list(mean_dict.values())\n",
    "    \n",
    "    transp_stdev_list = np.array(stdev_list).T\n",
    "    sig = statistics.stdev(transp_stdev_list.reshape(-1))\n",
    "    mu = sum(transp_stdev_list.reshape(-1))/len(transp_stdev_list.reshape(-1))\n",
    "    thresh = mu+sig*2.5\n",
    "    print(thresh)\n",
    "    \n",
    "    \n",
    "    trade_action_list = []\n",
    "    trade_return_list = []\n",
    "    trained_model = models[0]\n",
    "    for j in range(250):\n",
    "        print((j))\n",
    "        action_tmp = []\n",
    "        reward_tmp = []\n",
    "        for i in range(len(reward_by_list)):\n",
    "            trained_model = models[i]\n",
    "            trade_df_daily_return, trade_df_actions = trade_rl(org_df,org_trade_start,org_trade_end,trained_model,org_initial_amount)\n",
    "            action_tmp.append(trade_df_actions)\n",
    "            trade_df_daily_return['daily_return']+=1\n",
    "            reward_tmp.append(trade_df_daily_return)\n",
    "    \n",
    "        # trade_dates = list(trade_df_daily_return['date'])\n",
    "\n",
    "        df_concat = pd.concat(action_tmp)\n",
    "        by_row_index = df_concat.groupby(df_concat.index)\n",
    "        final_action = by_row_index.mean()\n",
    "        trade_action_list.append(final_action)\n",
    "        \n",
    "        df_concat = pd.concat(reward_tmp)\n",
    "        by_row_index = df_concat.groupby(df_concat.index)\n",
    "        final_reward = by_row_index.mean()-1\n",
    "        trade_return_list.append(final_reward)\n",
    "        \n",
    "    df_concat = pd.concat(trade_action_list)\n",
    "    by_row_index = df_concat.groupby(df_concat.index)\n",
    "    mean_df = by_row_index.mean()\n",
    "    stdev_df = by_row_index.std()\n",
    "    \n",
    "    detect = []\n",
    "    for d in stdev_df.T:\n",
    "        for etf in stdev_df.columns:\n",
    "            stdev = stdev_df[etf][d]\n",
    "            mean = mean_df[etf][d]\n",
    "            if stdev>thresh:\n",
    "                detect.append([d,etf,stdev,mean])\n",
    "    print(detect)\n",
    "    \n",
    "    return detect,df_concat,trade_df_daily_return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def org_detect_func(org_etfs):\n",
    "    org_df = get_data(org_etfs)\n",
    "    models = []\n",
    "    \n",
    "    for i in range(len(reward_by_list)):\n",
    "        print(i)\n",
    "        trained_model = train_rl(org_df,org_train_start,org_train_end,org_initial_amount,reward_by_list[i])\n",
    "        models.append(trained_model)\n",
    "        trained_model.save('./models/model_'+reward_by_list+'.pt')\n",
    "    \n",
    "    detect,df_concat,trade_df_daily_return = detect_uncertainty(models,org_df)\n",
    "    \n",
    "    ddate = detect[0][0]\n",
    "    dtarget = detect[0][1]\n",
    "    textfile = open(\"./detect_record/mean_stdev.txt\", \"a\")\n",
    "    textfile.write('abnormal'+'\\t'+ddate+'\\t'+dtarget+\"\\n\")\n",
    "    \n",
    "    df_concat_new = df_concat.reset_index()\n",
    "    df_concat_cut = df_concat_new[df_concat_new['date']==detect[0][0]]\n",
    "    fig,ax=plt.subplots(1,len(org_etfs),figsize=(30,8))\n",
    "    i=0\n",
    "    for col in df_concat_cut.columns[1:]:\n",
    "        tttt = list(df_concat_cut[col])\n",
    "        stdev = statistics.stdev(tttt)\n",
    "        mean = sum(tttt)/len(tttt)\n",
    "        print(i,col,stdev,mean)\n",
    "        textfile.write(col+'\\t'+str(stdev)+'\\t'+str(mean)+\"\\n\")\n",
    "        sns.distplot(tttt,ax = ax[i],bins=10 )\n",
    "        i+=1\n",
    "    plt.savefig('./fig/'+detect[0][0]+'_abnormal.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    textfile.close()\n",
    "\n",
    "    return models,detect,trade_df_daily_return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def find_new_target(old_etfs,detect,detect_latest,models,last_detect_date):\n",
    "\n",
    "    change_date = detect_latest[0]\n",
    "    change_etf = detect_latest[1]\n",
    "    change_etf_idx = old_etfs.index(change_etf)\n",
    "    change_date_org = str(change_date)\n",
    "    \n",
    "    change2org=False\n",
    "    if old_etfs!=org_etfs:\n",
    "        for i in range(len(org_etfs)):\n",
    "            etf = org_etfs[i]\n",
    "            now_etfs = old_etfs.copy()\n",
    "            print(etf,now_etfs)\n",
    "            if etf not in now_etfs:#org誰被換掉\n",
    "                print(etf+'not in comb')\n",
    "                for j in range(len(now_etfs)):\n",
    "                    if now_etfs[j] not in org_etfs:\n",
    "                        now_etfs[j] = etf\n",
    "                        break\n",
    "                print(now_etfs)\n",
    "                dt_tmp = last_detect_date.split('-')\n",
    "                dt = datetime.date(int(dt_tmp[0]),int(dt_tmp[1]),int(dt_tmp[2]))\n",
    "                time_del = datetime.timedelta(days=1) \n",
    "                dt = dt+time_del\n",
    "                dt_tmp = change_date.split('-')\n",
    "                dt_stop = datetime.date(int(dt_tmp[0]),int(dt_tmp[1]),int(dt_tmp[2]))\n",
    "                print(dt,dt_stop)\n",
    "                while True:\n",
    "                    if dt==dt_stop:\n",
    "                        break\n",
    "                    if str(dt) not in trade_dates:\n",
    "                        print(dt,'no trade')\n",
    "                        dt = dt+time_del\n",
    "                        continue\n",
    "                    avg_reward,avg_std,avg_mdd = get_avg_ABC(now_etfs,org_trade_start,str(dt))\n",
    "#                     if avg_reward>0.045 and avg_std<0.0832 and avg_mdd<0.1103:\n",
    "                    if avg_reward>0.063 and avg_std<0.113 and avg_mdd<0.145:#這裡要手調\n",
    "                        print(dt,'abc ok')\n",
    "                        change2org=True\n",
    "                        break\n",
    "                    dt = dt+time_del\n",
    "                    print(dt)\n",
    "            if change2org:\n",
    "                break\n",
    "                \n",
    "    change_success = False\n",
    "    for etf in all_etf:\n",
    "        if not change2org:\n",
    "            now_etfs = old_etfs.copy()\n",
    "            change_date = change_date_org\n",
    "            if etf not in now_etfs:\n",
    "                now_etfs[change_etf_idx] = etf\n",
    "                \n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "        else:\n",
    "            change2org=False\n",
    "            change_date = str(dt)\n",
    "\n",
    "        print(now_etfs)\n",
    "        avg_reward,avg_std,avg_mdd = get_avg_ABC(now_etfs,'2016-01-01',change_date)\n",
    "        if avg_reward>0.06 and avg_std<0.12 and avg_mdd<0.15:#0.0862 0.1133 #這裡要手調\n",
    "#         if avg_reward>0.0415 and avg_std<0.1 and avg_mdd<0.13:#0.0862 0.1133\n",
    "            print('abc ok')\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "\n",
    "        now_df = get_data(now_etfs)\n",
    "        print(now_df.tic.unique())\n",
    "        if len(now_df.tic.unique())!=len(org_etfs):\n",
    "            continue\n",
    "\n",
    "        detect_new,df_concat,_ = detect_uncertainty(models,now_df)\n",
    "        \n",
    "        #如果非異常 跳出迴圈 回傳這個組合與模型\n",
    "        flag = True\n",
    "        detect_latest = []\n",
    "        for i in range(len(detect_new)):\n",
    "            dt_tmp = change_date.split('-')\n",
    "            dt = datetime.date(int(dt_tmp[0]),int(dt_tmp[1]),int(dt_tmp[2]))\n",
    "            \n",
    "            dt_tmp = detect_new[i][0].split('-')\n",
    "            dt_new = datetime.date(int(dt_tmp[0]),int(dt_tmp[1]),int(dt_tmp[2]))\n",
    "            \n",
    "            if dt==dt_new and detect_new[i][1]==change_etf: #if detect_new[i][0]==change_date:\n",
    "                flag=False\n",
    "                print('detect_new[i]',detect_new[i])\n",
    "                break\n",
    "            elif dt==dt_new:\n",
    "                detect_latest = detect_new[i]\n",
    "                print('detect_latest',detect_latest)\n",
    "                break\n",
    "            elif dt_new>dt: #dt_new較晚\n",
    "                detect_latest = detect_new[i]\n",
    "                print('detect_latest',detect_latest)\n",
    "                break\n",
    "               \n",
    "        if flag:\n",
    "            change_success=True\n",
    "            print(now_etfs,'successed','normal@',change_date)\n",
    "            # plot ok distribution\n",
    "            textfile = open(\"./detect_record/mean_stdev.txt\", \"a\")\n",
    "            textfile.write('normal'+'\\t'+change_date+\"\\n\")\n",
    "            \n",
    "            df_concat_new = df_concat.reset_index()\n",
    "            df_concat_cut = df_concat_new[df_concat_new['date']==change_date]\n",
    "            fig,ax=plt.subplots(1,len(now_etfs),figsize=(30,8))\n",
    "            i=0\n",
    "            for col in df_concat_cut.columns[1:]:\n",
    "                print(df_concat_cut[col])\n",
    "                tttt = list(df_concat_cut[col])\n",
    "                stdev = statistics.stdev(tttt)\n",
    "                mean = sum(tttt)/len(tttt)\n",
    "                print(i,col,stdev,mean)\n",
    "                textfile.write(col+'\\t'+str(stdev)+'\\t'+str(mean)+\"\\n\")\n",
    "                sns.distplot(tttt,ax = ax[i],bins=10 )\n",
    "                i+=1\n",
    "            plt.savefig('./fig/'+change_date+'_'+change_etf+'_ok.png')\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "#             textfile.close()\n",
    "  \n",
    "            try:\n",
    "                # plot abnormal distribution\n",
    "                textfile.write('abnormal'+'\\t'+detect_latest[0]+'\\t'+detect_latest[1]+\"\\n\")\n",
    "            \n",
    "                df_concat_new = df_concat.reset_index()\n",
    "                df_concat_cut = df_concat_new[df_concat_new['date']==detect_latest[0]]\n",
    "                fig,ax=plt.subplots(1,len(now_etfs),figsize=(30,8))\n",
    "                i=0\n",
    "                for col in df_concat_cut.columns[1:]:\n",
    "                    tttt = list(df_concat_cut[col])\n",
    "                    stdev = statistics.stdev(tttt)\n",
    "                    mean = sum(tttt)/len(tttt)\n",
    "                    print(i,col,stdev,mean)\n",
    "                    textfile.write(col+'\\t'+str(stdev)+'\\t'+str(mean)+\"\\n\")\n",
    "                    sns.distplot(tttt,ax = ax[i],bins=10 )\n",
    "                    i+=1\n",
    "                plt.savefig('./fig/'+detect_latest[0]+'_'+detect_latest[1]+'_abnormal.png')\n",
    "                plt.show()\n",
    "                plt.close()\n",
    "                textfile.close()\n",
    "            \n",
    "                break\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            print(now_etfs,'failed')\n",
    "    if change_success:\n",
    "        return models,now_etfs,detect_new,detect_latest,change_date\n",
    "    else:\n",
    "        return models,None,None,None,None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新生成2001就存在的\n",
    "filename = 'all_us_etf.txt'#\n",
    "\n",
    "all_etf = []  \n",
    "f = open(filename)\n",
    "for line in f:\n",
    "#     print(line[:-1])\n",
    "    all_etf.append(line[:-1])\n",
    "print(len(all_etf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化一些參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_by_list=['reward','mdd','1mstdev']\n",
    "org_etfs = ['ITOT', 'VEU', 'VNQ', 'AGG']\n",
    "# org_etfs = ['VTI','TLT','IEF','GLD','DBC']\n",
    "org_train_start = '2009-01-01'\n",
    "org_train_end = '2016-01-01'\n",
    "org_trade_start = '2016-01-01' \n",
    "org_trade_end = '2021-01-01'\n",
    "org_initial_amount = 1000000\n",
    "\n",
    "etf_record = [org_etfs]\n",
    "detect_record = []\n",
    "detect_date_record = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練模型+第一次偵測+儲存資訊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models,org_detect,trade_df_daily_return_org = org_detect_func(org_etfs)\n",
    "trade_dates = list(trade_df_daily_return_org['date'])\n",
    "detect = org_detect.copy()\n",
    "detect_record.append(detect)\n",
    "detect_date_record.append(detect[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textfile = open(\"./detect_record/detect_record.txt\", \"w\")\n",
    "textfile.write(\"00\"+'\\n')\n",
    "for dtct_r in detect:\n",
    "    for kkk in range(len(dtct_r)):\n",
    "        element = dtct_r[kkk]\n",
    "#         print(element)\n",
    "        textfile.write(str(element))\n",
    "        if kkk<len(dtct_r)-1:\n",
    "            textfile.write(\"\\t\")\n",
    "        else:\n",
    "            textfile.write(\"\\n\")\n",
    "textfile.close()\n",
    "\n",
    "textfile = open(\"./detect_record/detect_date_record.txt\", \"w\")\n",
    "textfile.write(detect[0][0]+'\\n')\n",
    "textfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 開始偵測+更換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 還沒解決如果都找不到 回傳None的問題\n",
    "old_etfs = org_etfs.copy()\n",
    "detect = detect_record[0]\n",
    "detect_latest = detect[0]\n",
    "last_detect_date = '2016-01-04'# test 第一天日期\n",
    "# print(detect)\n",
    "count=1\n",
    "while True:\n",
    "    random.shuffle(all_etf)\n",
    "    print('old_etfs:',old_etfs)\n",
    "    print('detect',detect)\n",
    "    trained_model,now_etfs,detect_new,detect_latest,last_detect_date = find_new_target(old_etfs,detect,detect_latest,trained_model,last_detect_date)\n",
    "    detect_record.append(detect_new)\n",
    "    detect_date_record.append(detect_latest[0])\n",
    "    etf_record.append(now_etfs)\n",
    "    \n",
    "#     last_detect_date = detect_latest[0]\n",
    "    old_etfs = now_etfs.copy()\n",
    "    detect = detect_new\n",
    "    \n",
    "    textfile = open(\"./detect_record/detect_record.txt\", \"a\")\n",
    "    if count<10:\n",
    "        textfile.write('0'+str(count)+'\\n')\n",
    "    else:\n",
    "        textfile.write(str(count)+'\\n')\n",
    "    for dtct_r in detect:\n",
    "        for kkk in range(len(dtct_r)):\n",
    "            element = dtct_r[kkk]\n",
    "#             print(element)\n",
    "            textfile.write(str(element))\n",
    "            if kkk<len(dtct_r)-1:\n",
    "                textfile.write(\"\\t\")\n",
    "            else:\n",
    "                textfile.write(\"\\n\")\n",
    "    textfile.close()\n",
    "    \n",
    "    textfile = open(\"./detect_record/detect_date_record.txt\", \"a\")\n",
    "    textfile.write(detect_latest[0]+'\\n')\n",
    "    textfile.close()\n",
    "    \n",
    "    count+=1\n",
    "    if detect_latest==[]:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 都換完了 讀取更換的時間和標的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './detect_record/mean_stdev.txt'#\n",
    "\n",
    "\n",
    "f = open(filename)\n",
    "flag = False\n",
    "dt_rcrd = []\n",
    "etf_rcrd = []\n",
    "tmp=[]\n",
    "for line in f:\n",
    "#     print(line[:-1])\n",
    "    ttt = line[:-1]\n",
    "    ttt_list = ttt.split('\\t')\n",
    "    print(ttt_list)\n",
    "    if flag:\n",
    "        tmp.append(ttt_list[0])\n",
    "        print(tmp)\n",
    "        if len(tmp)==4:\n",
    "            etf_rcrd.append(tmp)\n",
    "            tmp=[]\n",
    "    if ttt_list[0]=='normal':\n",
    "#         print('normal')\n",
    "        flag=True\n",
    "        tmp=[]\n",
    "        dt_rcrd.append(ttt_list[1])\n",
    "    if ttt_list[0]=='abnormal':\n",
    "        flag = False\n",
    "print(dt_rcrd)\n",
    "print(etf_rcrd)\n",
    "print(len(dt_rcrd),len(etf_rcrd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 每個時間和組合都重新trade一次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_action_list = []\n",
    "trade_return_list = []\n",
    "for i in range(len(etf_rcrd)):#etf_record\n",
    "    now_etfs = etf_rcrd[i]\n",
    "    now_df = get_data(now_etfs)\n",
    "    print(now_df.tic.unique())\n",
    "    trade_df_daily_return, trade_df_actions = trade_rl(now_df,org_trade_start,org_trade_end,trained_model,org_initial_amount)\n",
    "    trade_action_list.append(trade_df_actions)\n",
    "    trade_return_list.append(trade_df_daily_return)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### combine成一個return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concate = []\n",
    "detect_date_record_new = dt_rcrd.copy()#detect_date_record\n",
    "detect_date_record_new.insert(0, \"2016-01-04\")\n",
    "for i in range(len(detect_date_record_new)):\n",
    "    if i==0:\n",
    "        trade_df_daily_return_org = trade_return_list[i]\n",
    "    if i!=len(detect_date_record_new)-1:\n",
    "        trade_start = detect_date_record_new[i] \n",
    "        trade_end = detect_date_record_new[i+1]\n",
    "        trade_df_daily_return = trade_return_list[i]\n",
    "        start_idx = trade_df_daily_return[trade_df_daily_return['date'] == trade_start].index.tolist()[0]\n",
    "        end_idx = trade_df_daily_return[trade_df_daily_return['date'] == trade_end].index.tolist()[0]\n",
    "        df_toconcate = trade_df_daily_return[start_idx:end_idx]\n",
    "    else:\n",
    "        trade_start = detect_date_record_new[i] \n",
    "        trade_df_daily_return = trade_return_list[i]\n",
    "        start_idx = trade_df_daily_return[trade_df_daily_return['date'] == trade_start].index.tolist()[0]\n",
    "        df_toconcate = trade_df_daily_return[start_idx:]\n",
    "    concate.append(df_toconcate)\n",
    "new_trade_df_daily_return = pd.concat(concate,axis=0,ignore_index = True)\n",
    "new_trade_df_daily_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 輸出績效"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRL_strat = convert_daily_return_to_pyfolio_ts(new_trade_df_daily_return)\n",
    "perf_func = timeseries.perf_stats \n",
    "perf_stats_all = perf_func( returns=DRL_strat, \n",
    "                              factor_returns=DRL_strat, \n",
    "                                positions=None, transactions=None, turnover_denom=\"AGB\")\n",
    "print(\"==============New DRL Strategy Stats===========\")\n",
    "perf_stats_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRL_strat = convert_daily_return_to_pyfolio_ts(pd.read_csv(('./csv/classic_org.csv'))[['date','daily_return']])#org_trade_df_daily_return\n",
    "perf_func = timeseries.perf_stats \n",
    "perf_stats_all = perf_func( returns=DRL_strat, \n",
    "                              factor_returns=DRL_strat, \n",
    "                                positions=None, transactions=None, turnover_denom=\"AGB\")\n",
    "print(\"==============Org DRL Strategy Stats===========\")\n",
    "perf_stats_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now_df = get_data(org_etfs)\n",
    "print(now_df.tic.unique())\n",
    "train_df_daily_return, train_df_actions = trade_rl(now_df,org_train_start,org_train_end,trained_model,org_initial_amount)\n",
    "    \n",
    "DRL_strat = convert_daily_return_to_pyfolio_ts(train_df_daily_return)\n",
    "perf_func = timeseries.perf_stats \n",
    "perf_stats_all = perf_func( returns=DRL_strat, \n",
    "                              factor_returns=DRL_strat, \n",
    "                                positions=None, transactions=None, turnover_denom=\"AGB\")\n",
    "print(\"==============Original DRL Strategy Stats @ train===========\")\n",
    "perf_stats_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRL_strat_org = convert_daily_return_to_pyfolio_ts(trade_df_daily_return_org)\n",
    "\n",
    "DRL_strat = convert_daily_return_to_pyfolio_ts(new_trade_df_daily_return)\n",
    "\n",
    "print(\"==============DRL Strategy Stats===========\")\n",
    "#     print(perf_stats_all)\n",
    "    \n",
    "with pyfolio.plotting.plotting_context(font_scale=1.1):\n",
    "    pyfolio.create_full_tear_sheet(returns = DRL_strat,\n",
    "                                       benchmark_rets=DRL_strat_org, set_context=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 存df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_trade_df_daily_return.to_csv('./csv/classic_bound_change_new.csv')\n",
    "trade_return_list[0].to_csv('./csv/classic_bound_org.csv')\n",
    "for i in range(len(trade_return_list)):\n",
    "    if i<10:\n",
    "        iiiii = '0'+str(i)\n",
    "    else:\n",
    "        iiiii = str(i)\n",
    "    trade_return_list[0].to_csv('./csv/classic_bound_new_'+iiiii+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aa]",
   "language": "python",
   "name": "aa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
